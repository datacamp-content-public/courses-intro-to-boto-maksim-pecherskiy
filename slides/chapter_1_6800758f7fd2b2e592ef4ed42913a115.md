---
title: Insert title here
key: 6800758f7fd2b2e592ef4ed42913a115

---
## Working with objects in buckets

```yaml
type: "TitleSlide"
key: "dd7c97db8a"
```

`@lower_third`

name: Maksim Pecherskiy
title: Chief Data Officer, City of San Diego


`@script`
In the last set of exercises, you learned how to list, create and delete buckets.  This will let you create custom containers as part of your data pipeline.  Now that we have these buckets, it's time to put stuff in them.  The files within AWS buckets are called objects.  And an object can really be anything - an image, a video file, CSVs, or log files.  Working with files in S3 is where Boto becomes really useful for your data pipeline work.  Uploading, downloading, and iterating over objects in S3 buckets is a key component of many pipelines.  Let's take a look at how objects work.


---
## How buckets and objects work together.

```yaml
type: "TwoColumns"
key: "5b3c400989"
```

`@part1`
- Object has only one parent bucket {{2}}
- Object knows about its parent bucket {{3}}
- Buckets contain many objects {{4}}
- Buckets can list their own objects {{5}}


`@part2`
![Bucket with objects](http://take.ms/Cvy4Z)


`@script`
Objects in AWS and in Boto are sub resources.  
That means that every object has one parent bucket.  
An object can't exist without a bucket.  

Objects know about their parent bucket and can get a reference to it.

Buckets can contain many objects, and they know how to list their own objects.


---
## Creating an object in a bucket.

```yaml
type: "TwoColumns"
key: "1bbc4a6e51"
center_content: false
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Create pointer to bucket**{{2}}
```python
trout_bucket = s3.Bucket(
    'datacamp-trout')
```{{2}}

**Upload!**{{3}}
```python
trout_bucket.upload_file(
     Filename='./trout_obs.csv', 
     Key='trout_obs_2019-01-27.csv')
```{{3}}


`@part2`
![](http://take.ms/znhsj){{4}}


`@script`
Let's take the empty bucket we created in the last set of exercises.  You may remember the cool name we gave it too - datacamp-trout.  Continuing along with our fish monitoring example, when you walk into work every morning, there's a file in your email.  You want to take that file and upload it to S3 with the current date as a suffix.  

Let's assume you download that file into the directory where you have the python file you are working in.  

We create the Boto resource that allows us to interact with S3.

Next, we create a reference to the datacamp-trout bucket.

Finally, we upload the CSV from the hard drive to S3 by calling upload_file on the bucket.  The filename parameter refers to our local file, and the Key parameter refers to the location we want to place that object in S3. S3 object file names are called keys. We'll get into some cool things you can do with keys later on.


---
## Object Operations

```yaml
type: "TwoRowsTwoColumns"
key: "f9498f915c"
center_content: false
```

`@part1`
### Create / Update 
Hello{{2}}


`@part2`
### Read


`@part3`
### List


`@part4`
### Delete


`@script`
Just like buckets, S3 objects are also Boto objects, allowing us to perform operations them.  For


---
## Final Slide

```yaml
type: "FinalSlide"
key: "e91c9c8136"
```

`@script`


