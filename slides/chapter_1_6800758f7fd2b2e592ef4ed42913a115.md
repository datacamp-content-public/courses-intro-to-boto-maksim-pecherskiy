---
title: Insert title here
key: 6800758f7fd2b2e592ef4ed42913a115

---
## Working with objects in buckets

```yaml
type: "TitleSlide"
key: "dd7c97db8a"
```

`@lower_third`

name: Maksim Pecherskiy
title: Chief Data Officer, City of San Diego


`@script`
In the last set of exercises, you learned how to list, create and delete buckets.  

This will let you create containers as part of your data pipeline.  

Now that we have these buckets, it's time to put stuff in them. The files within S3 buckets are called objects.  

An object can really be anything - an image, a video file, CSV or a log file.  

Uploading, downloading, and getting object metadata is a key component of many data pipelines.  

Let's take a look at how objects work.


---
## How buckets and objects work together.

```yaml
type: "TwoColumns"
key: "5b3c400989"
```

`@part1`
- Object has only one parent bucket {{2}}
- Object knows about its parent bucket {{3}}
- Buckets contain many objects {{4}}
- Buckets can list their own objects {{5}}


`@part2`
![Bucket with objects](http://take.ms/Cvy4Z)


`@script`
Objects in AWS and in Boto are sub resources.  
That means that every object has one parent bucket.  
An object can't exist without a bucket.  

Objects know about their parent bucket and can get a reference to it.

Buckets can contain many objects, and they know how to list their own objects. 

Let's look at how we can upload and download objects.


---
## Creating or updating an object in a bucket.

```yaml
type: "TwoColumns"
key: "1bbc4a6e51"
center_content: false
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Create reference to the object to be created**{{2}}
```python
trout_file_pointer = s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv')
```{{2}}

**Upload / Overwrite**{{3}}
```python
trout_file_pointer.upload_file(
     Filename='./trout_obs.csv')
```{{3}}


`@part2`
![](http://take.ms/znhsj){{4}}


`@script`
Let's take the empty bucket we created in the last set of exercises.  You may remember the cool name we gave it too - datacamp-trout.  Continuing along with our fish monitoring example, when you walk into work every morning, there's a file in your email.  You want to take that file and upload it to S3 with the current date as a suffix.  

Let's assume you download that file into the directory where you have the python file you are working in.  

First we create our resource so we can work  with S3.

Then we create a reference to the file to be created in S3. We give it the bucket name and "key" as parameters. The key is what we want to name the bucket on S3.

Note that this does not mean there is a file there yet.  It's just a pointer, but right now it points to an empty space.

Then, we fill that pointer up with the contents of our trout_obs.csv file by calling upload file, and giving the local file name as a parameter.  

Now we  can see that the file has been uploaded to the bucket!

It's important to keep in mind that since it's the same operation to upload and create, if a file already exists at the key you specified, it will get overwritten!


---
## Downloading an object from a bucket.

```yaml
type: "FullCodeSlide"
key: "5557c943e1"
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Create reference to the object to be created**{{2}}
```python
trout_file_pointer = s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv')
```{{2}}

**Download!**{{3}}
```python
trout_file_pointer.download_file(
     Filename='./trout_obs_downloaded_from_s3.csv')
```{{3}}


`@script`
Downloading is very similar to uploading.

We create the resource pointer. 
Then, we make another pointer, this time to a  file that already  exists.

Then, we download, specifying where we want the file to land locally.


---
## What can you do with a reference?

```yaml
type: "TwoColumns"
key: "698ef7c212"
```

`@part1`
** Get Size **{{1}}
```python
trout_file_pointer.content_length
# 6348635
``` {{1}}

** Get Bucket Name **{{2}}
```python
trout_file_pointer.bucket_name
# datacamp-trout
```{{2}}

** Get Last Modified **{{3}}
```python
trout_file_pointer.last_modified
# datetime.datetime(2019, 1, 27, 22, 16, 17, tzinfo=tzutc())
```{{3}}


`@part2`
** Get Bucket Reference **{{4}}
```python
bucket = trout_file_pointer.Bucket
# Now we can use the bucket variable 
like the previous lesson
```{{4}}


[Full metadata list](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#object)!{{5}}


`@script`
So why do we even need the functionality to create a pointer to an object in S3, especially if it doesn't exist yet?  

There are many reasons, some of which we will get into later on, but for now let's look at one -- the juicy metadata we can get about the object from its reference.

We can get the object size.
The name of the bucket the object is in.
A datetime object of when it was last modified.
And best of all, we can get a reference to its parent bucket to perform all the operations we learned in the previous lesson.


---
## Deleting an object from a bucket.

```yaml
type: "FullCodeSlide"
key: "0faa2b35a6"
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Delete!**{{2}}
```python
s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv').delete()
```{{2}}


`@script`
Deleting is very similar as well.  Except this time, let's chain our call together so our code is more concise.  

Instead of creating a reference to the file and  storing it in a variable, we simply create the reference and immediately call delete() on the file.  

Now our file is gone!


---
## Let's set some permissions!

```yaml
type: "FinalSlide"
key: "e91c9c8136"
```

`@script`
You learned how to create, update delete and download objects.  

Now, let's take a look how to control who can do what in our buckets!

