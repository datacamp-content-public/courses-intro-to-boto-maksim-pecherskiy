---
title: Insert title here
key: 6800758f7fd2b2e592ef4ed42913a115

---
## Working with objects in buckets

```yaml
type: "TitleSlide"
key: "dd7c97db8a"
```

`@lower_third`

name: Maksim Pecherskiy
title: Chief Data Officer, City of San Diego


`@script`
In the last set of exercises, you learned how to list, create and delete buckets.  

This will let you create containers as part of your data pipeline.  

Now that we have these buckets, it's time to put stuff in them. The files within S3 buckets are called objects.  

An object can really be anything - an image, a video file, CSV or a log file.  

Uploading, downloading, and getting object metadata is a key component of many data pipelines.  

Let's take a look at how objects work.


---
## How buckets and objects work together.

```yaml
type: "TwoColumns"
key: "5b3c400989"
```

`@part1`
- Buckets contains many objects {{1}}
- Objects have only one parent bucket {{1}}
- Buckets can list their children objects {{2}}
- Objects can access their parent bucket {{2}}
- Buckets have unique names {{3}}
- Objects have unique keys {{3}}


`@part2`
![Bucket with objects](http://take.ms/Cvy4Z){{1}}


`@script`
Objects and buckets in S3 work very similarly to a desktop file system.  They are sub-resources of each other.

Buckets contain many objects, but an object can only belong to one bucket.  

--

Buckets can iterate over their children objects, and objects can access their parent buckets.

--

Buckets have unique names, and objects have unique keys (aka filenames) in a bucket.

--

Let's look at how some specific object operations.


---
## Creating objects in buckets.

```yaml
type: "TwoColumns"
key: "1bbc4a6e51"
center_content: false
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Create reference to the object to be created**{{2}}
```python
trout_file = s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv')
```{{2}}

**Upload**{{3}}
```python
trout_file.upload_file(
     Filename='./trout_obs.csv')
```{{3}}


`@part2`
![](http://take.ms/znhsj){{4}}


`@script`
Let's take the empty bucket we created in the last lesson. You may remember the cool name we gave it - datacamp-trout. In this magical fish-monitoring place where you work, you walk into work every morning to find a file in your e-mail with the latest trout monitoring data. Let's take that file and upload it to S3 with the current date as a suffix of the key. 


First we create our resource.

--

Then we create a reference to the file we are going to create. We pass bucket name and "key" as parameters. The key is what we want to name the file on S3.

--

This does not mean there is a file there yet. It's just a reference, and right now it points to an empty space.

Next, let's fill that empty space with the contents of our trout_obs.csv. We call upload file, an pass the local file name as parameter.  

--

Now we  can see that the file is uploaded.

--

It's important to keep in mind that since it's the same operation to upload and create, if a file already exists at the key you specified, it will get overwritten! But that's also good for when you want to update a file.


---
## Downloading an object from a bucket.

```yaml
type: "FullCodeSlide"
key: "5557c943e1"
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Create reference to the object to be created**{{2}}
```python
trout_file = s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv')
```{{2}}

**Download!**{{3}}
```python
trout_file.download_file(
     Filename='./trout_obs_downloaded_from_s3.csv')
```{{3}}


`@script`
Downloading is very similar to uploading.

We create the resource. 

--

Then, we make another reference, this time to a file that already  exists.

--

Then, we download, specifying where we want the file to land locally.


---
## What can you do with a reference?

```yaml
type: "TwoColumns"
key: "698ef7c212"
```

`@part1`
** Get Size **{{1}}
```python
trout_file.content_length
# 6348635
``` {{1}}

** Get Bucket Name **{{2}}
```python
trout_file.bucket_name
# datacamp-trout
```{{2}}

** Get Last Modified **{{3}}
```python
trout_file.last_modified
# datetime.datetime(2019, 1, 27, 22, 16, 17, tzinfo=tzutc())
```{{3}}


`@part2`
** Get Bucket Reference **{{4}}
```python
bucket = trout_file.Bucket
# Now we can use the bucket variable 
like the previous lesson
```{{4}}


[Full metadata list](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#object)!{{5}}


`@script`
So why do we need the functionality to create a reference to an object in S3, even if it doesn't exist?  

There are many reasons, but for now let's look at one. The juicy metadata we can get about the object.

We can get the object size.

--

The name of the bucket the object is in.

--

A datetime of when it was last modified.

--

And best of all, we can get a reference to its parent bucket to perform all the operations we learned in the previous lesson.

--


---
## Deleting an object from a bucket.

```yaml
type: "FullCodeSlide"
key: "0faa2b35a6"
```

`@part1`
**Create resource** {{1}}
```python
s3 = boto3.resource('s3')
```{{1}}

**Delete!**{{2}}
```python
s3.Object(
    bucket_name='datacamp-trout',
    key='trout_obs_2019-01-27.csv').delete()
```{{2}}


`@script`
Deleting is very similar as well.  Except this time, let's chain our call together so our code is more concise.  

Instead of creating a reference to the file and  storing it in a variable, we simply create the reference and immediately call delete() on the file.  

Now our file is gone!


---
## Let's set some permissions!

```yaml
type: "FinalSlide"
key: "e91c9c8136"
```

`@script`
You learned how to create, update delete and download objects.  

Now, let's take a look how to control who can do what in our buckets!

